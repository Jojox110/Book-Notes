# Starting a machine learning project
https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb

See appendix_a.md for ML project checklist

## Frame the problem
Signal: "A piece of information fed to a machine learning sytstem is often called a *signal*"

Pipeline: A series of data processing components. See p.42

-> Doing this allows you to have a better idea of what you're being asked to do and how you will complete the task

-> Getting a better idea of why you are being asked to do said task will help you plan

## Select a performance mesure

### Notations:
- m = the number of instances in the dataset
- x(i) is a vector containing the information about the features
- y(i) contains the label
- X is a matrix that contains all the feature values
- h is the system's prediction system

## RMSE
See p.43 for equation

RMSE is an equation that calculates the performance of a model, but is usually used in regression tasks. RMSE follows the l2 norm and will give a higher weight to bigger errors

In more details, RMSE uses the Euclidian distance to calculate the distance between the target vector and the prediction vector.

## MAE
See p.45 for equation

Just like RMSE, MAE calculates the distance between the prediction vector and the target vector, but it uses the Manhattan distance instead. MAE is also an l1 norm, which means it doesn't give as much weight to bigger errors like RMSE does.

## lk norms
-> What is the letter? Weird p or weird cursive l ??

When talking about lk norms, a higher index, k, means that the function will give more weight to bigger errors, and the opposite for a smaller k. Which one you will use depends on what exactly you're trying to measure.

## Downloading the data using an automated Python function
```py
from pathlib import Path
import pandas as pd
import tarfile
import urllib.request

def load_housing_data():
    tarball_path = Path("datasets/housing.tgz")
    if not tarball_path.is_file():
        Path("datasets").mkdir(parents=True, exist_ok=True)
        url = "https://github.com/ageron/data/raw/main/housing.tgz"
        urllib.request.urlretrieve(url, tarball_path)  # Download the data
        with tarfile.open(tarball_path) as housing_tarball:
            housing_tarball.extractall(path="datasets") # Extract from the tar.gz
    return pd.read_csv(Path("datasets/housing/housing.csv"))

housing = load_housing_data() # Load into a pd dataframe
```

## Creating a test set

```py
import numpy as np

def shuffle_and_split_data(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]

train_set, test_set = shuffle_and_split_data(housing, 0.2)
len(train_set)
```

### np.random.permutation
This method returns a random permutation of the given data. We passed a number in this case, so it will make a permutations of the range of that number

### test_set_size, test_indices and train_indices
test_set_indices decides how many indices is equal to the test_ratio, so in this case, 20640 * 0.2\
test_indices and train_indices take in the first 20% and the last 80% of the data. The data that it is taking is the one shuffled by np.random.permutation\
the function then returns the rows selected by train_indices and test_indices

### Using a hash shuffle and split method
Using a hash shuffle a split method allows us to assure that the training and test data are always seperated even if we feed more data into the dataset.
```py
from zlib import crc32

def is_id_in_test_set(identifier, test_ratio):
    return crc32(np.int64(identifier)) < test_ratio * 2**32

def split_data_with_id_hash(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))
    return data.loc[in_test_set], data.loc[in_test_set]
```

Having those randomized test/train split algorithms are good, but since it's randomized, they can introduce some sampling bias. This is where StraifiedShuffleSplit can be used. To do this, you can create a new category in the data using pd.cut and using big enough stratums (using bins), you can categorize an important factor. With a categorized column and StratifiedShuffleSplit, we can make sure that the data is less biased. This is because it'll split the categorized data to get almost even distribution of data in the training and in the test datasets.

```py
housing["income_cat"] = pd.cut(housing["median_income"],
                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
                               labels=[1, 2, 3, 4, 5])



from sklearn.model_selection import StratifiedShuffleSplit

splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
strat_splits = []
for train_index, test_index in splitter.split(housing, housing["income_cat"]):
    strat_train_set_n = housing.iloc[train_index]
    strat_test_set_n = housing.iloc[test_index]
    strat_splits.append([strat_train_set_n, strat_test_set_n])

strat_train_set, strat_test_set = strat_splits[0]

# To remove the column
for set_ in (strat_train_set, strat_test_set):
    set_.drop("income_cat", axis=1, inplace=True)
```

## Exploring the data
When manipulating the data for visualization, it's always a good idea to be working on a copy of the data
```py
housing = strat_train_set.copy()
```

### Visualizing Data
In a scatter plot, using the alpha parameter helps to visualize the places with a high density of data points

For more details on how to visualize a plot better, see the parameters used on p.62

### Look for Correlations
In order to get a good view of the correlations for a specific column, we can use 
```py
print(variable_storing_corr["column"].sort_values(ascending=false))
``` 

Note: the pd.cor() method **only calculates linear correlation**

### Experiment with Attribute Combinations
In order to get a better idea of what is going on in our data, we can combine some closely related attributes to get a more useful attribute. For example, the amount of bedrooms and the amount of households seperate aren't super useful, but the amount of bedrooms per household can be more interesting.

## Prepare the Data for Machine Learning Algorithmns

### Clean the Data

### Handling Text and Categorical Attributes

### Feature Scaling and Transformation

### Custom Transformers

### Transformation Pipelines

## Select and Train a Model

### Train and Evaluate on the Training set

### Better Evaluation Using Cross-Validation

## Fine-Tune Your Model

### Grid Search

### Randomized Search

### Ensemble Methods

### Analyzing the Best Models and Their Errors

### Evaluate Your System on the Test Set

## Launch, Monitor, And Maintain Your System