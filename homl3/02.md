# Starting a machine learning project
https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb

See appendix_a.md for ML project checklist

## Frame the problem
Signal: "A piece of information fed to a machine learning sytstem is often called a *signal*"

Pipeline: A series of data processing components. See p.42

-> Doing this allows you to have a better idea of what you're being asked to do and how you will complete the task

-> Getting a better idea of why you are being asked to do said task will help you plan

## Select a performance mesure

### Notations:
- m = the number of instances in the dataset
- x(i) is a vector containing the information about the features
- y(i) contains the label
- X is a matrix that contains all the feature values
- h is the system's prediction system

## RMSE
See p.43 for equation

RMSE is an equation that calculates the performance of a model, but is usually used in regression tasks. RMSE follows the l2 norm and will give a higher weight to bigger errors

In more details, RMSE uses the Euclidian distance to calculate the distance between the target vector and the prediction vector.

## MAE
See p.45 for equation

Just like RMSE, MAE calculates the distance between the prediction vector and the target vector, but it uses the Manhattan distance instead. MAE is also an l1 norm, which means it doesn't give as much weight to bigger errors like RMSE does.

## lk norms
-> What is the letter? Weird p or weird cursive l ??

When talking about lk norms, a higher index, k, means that the function will give more weight to bigger errors, and the opposite for a smaller k. Which one you will use depends on what exactly you're trying to measure.

## Downloading the data using an automated Python function
```py
from pathlib import Path
import pandas as pd
import tarfile
import urllib.request

def load_housing_data():
    tarball_path = Path("datasets/housing.tgz")
    if not tarball_path.is_file():
        Path("datasets").mkdir(parents=True, exist_ok=True)
        url = "https://github.com/ageron/data/raw/main/housing.tgz"
        urllib.request.urlretrieve(url, tarball_path)  # Download the data
        with tarfile.open(tarball_path) as housing_tarball:
            housing_tarball.extractall(path="datasets") # Extract from the tar.gz
    return pd.read_csv(Path("datasets/housing/housing.csv"))

housing = load_housing_data() # Load into a pd dataframe
```

## Creating a test set

```py
import numpy as np

def shuffle_and_split_data(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]

train_set, test_set = shuffle_and_split_data(housing, 0.2)
len(train_set)
```

### np.random.permutation
This method returns a random permutation of the given data. We passed a number in this case, so it will make a permutations of the range of that number

### test_set_size, test_indices and train_indices
test_set_indices decides how many indices is equal to the test_ratio, so in this case, 20640 * 0.2\
test_indices and train_indices take in the first 20% and the last 80% of the data. The data that it is taking is the one shuffled by np.random.permutation\
the function then returns the rows selected by train_indices and test_indices

### Using a hash shuffle and split method
Using a hash shuffle a split method allows us to assure that the training and test data are always seperated even if we feed more data into the dataset.
```py
from zlib import crc32

def is_id_in_test_set(identifier, test_ratio):
    return crc32(np.int64(identifier)) < test_ratio * 2**32

def split_data_with_id_hash(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]
```