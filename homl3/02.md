# Starting a machine learning project

## Creating a test set

```py
import numpy as np

def shuffle_and_split_data(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]

train_set, test_set = shuffle_and_split_data(housing, 0.2)
len(train_set)
```

### np.random.permutation
This method returns a random permutation of the given data. We passed a number in this case, so it will make a permutations of the range of that number

### test_set_size, test_indices and train_indices
test_set_indices decides how many indices is equal to the test_ratio, so in this case, 20640 * 0.2\
test_indices and train_indices take in the first 20% and the last 80% of the data. The data that it is taking is the one shuffled by np.random.permutation\
the function then returns the rows selected by train_indices and test_indices

### Using a hash shuffle and split method
Using a hash shuffle a split method allows us to assure that the training and test data are always seperated even if we feed more data into the dataset.
```py
from zlib import crc32

def is_id_in_test_set(identifier, test_ratio):
    return crc32(np.int64(identifier)) < test_ratio * 2**32

def split_data_with_id_hash(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))
    return data.loc[~in_test_set], data.loc[in_test_set]
```


