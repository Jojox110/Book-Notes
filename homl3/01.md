# Types of machine learning algorithmns
https://github.com/ageron/handson-ml3/blob/main/01_the_machine_learning_landscape.ipynb

## Supervised Learning
Supervised learning applies to scenarios where the data is all labeled. That would mean that the training set that you provide to the algorithm is followed by each line of data's desired result

## Unsupervised Learning
In unsupervised learning, we basically give our algorithm a lot of unlabeled data and ask it to find patterns in the data. An example of a place where this is common is in clustering algorithms 

## Semi-supervised Learning
In semi-supervised learning, the algorithm is provided a dataset with unlabeled and labeled data. This is often gonna result in using supervised and unsupervised learning algorithms together

## Self-supervised Learning
A self-supervised learning algorithm takes in an unlabeled dataset, just like an unsupervised learning algorithm, and comes back with a labeled dataset. This learning technique is often used for regression and classification tasks

## Reinforcement Learning
Reinforcement learning is an algorithm that uses an agent to make actions. Those actions will either be compensated positively or negatively from which the agent will learn from

<br><br>

# Batch and online learning

## Batch Learning
Batch learning, aka offline learning, means that the algorithm you are training will be trained on a lot of data at once and that it will not be updated as it goes. This means that the model can become outdated to new data if not re-trained regularly, which depending on the case, can be ineffective and costly to do if it can even be done.

## Online Learning
Online learning means that the algorithm will be constantly fed new data on the fly and that it will be kept up to date. Online learning can also allow for incremental learning, which means loading the data bit by bit until you used up all the data (for cases where storage is an issue). But because the algorithm is constantly updated with new data, this can mean that it can forget older data or become vulnerable to bad data depending on the learning rate associated with the algorithm

<br><br>

# Instance-Based and Model-Based learning

## Instance-Based Learning
Instance-Based learning is when the algorithms learns the provided examples by heart and can compare them to new examples to come to a conclusion. For example, comparing a known spam email with an email that was just received

## Model-Based Learning
Model-Based learning is when the algorithms learns from the provided examples and creates a model that can be used to generalize off its provided data to assign a result to a new piece of data.

<br><br>

# Challenges of machine learning

## Insufficient Data
The amount of available data is a very important factor in the making of a machine learning model. Making a computer be able to complete a task, such as recognizing an apple for example, takes a whole lot of data. 

But, after a while, the amount of data starts taking over the algorithm in terms of importance. When working with smaller amounts of data, the algorithm you use is very important. But when you have huge amounts of data, the choice of algorithm becomes less important as some tests showed that the different models performance very similairly when you have enough data

## Non-representative training data
Having data that represents as many cases as possible is a big factor in whether your model is going to be accurate or not. Fro example, if most of the data is concentrated in one place, then the results are going to be biased to that data (sampling bias). See p.28 for example

## Poor quality data
Having poor quality data being fed to an algorithm can lead to it having unexpected behaviors. Anything from missing features to bad data can have negative effects on the model that is using said data.

## Irrelevant features
Having the proper features is a very important part of training an accurate model. This can be achieved with the help of feature engineering, which consists of *feature selection*, *feature extraction* and *creating new data*. See p.30

## Overfitting the training data
Overfitting the training data typically consists of choosing the wrong model or the wrong hyperparameter for the case. When we try to make a model, it's possible that it'll end up being biased towards the training data that we provided which will lead to it not being able to generalize to other cases very well. This can fixed by keeping a constant hyperparameter appropriate for the case.

Overfitting the data can also lead to the model detecting patterns that aren't what we're looking for or that is simply incaccurate. See p.30-31

## Underfitting the training data
Underfitting the data is the opposite of overfitting the data. This means that the model is too simple, maybe theres not enough features, a bad hyperparameter, etc

<br><br>

# Testing and validating
Testing and validating data mostly consists of testing the model using the testing set. Typically you would want 80% of the data to be used for training and the remaining 20% to be used for testing, but that depends on the context. If you have a whole lot of data, then having a much lower percentage go towards testing should still be able to give you a good idea of how well it performs

<br><br>

# Hyperparameter tuning and model selection
When you're trying to figure out which model works best for your case, there's a few steps to look out for to make sure that you're on the right track. First thing, it's a good idea to make a small validation set consisting of data that you took out of the training set. This can be used to evaluate the models before sending them off to the testing set. 

If you only use the testing set to choose the best model, then it's possible that you chose a model biased to the testing data and it might not generalize as well as expected. By having a validation set and a testing set, you reduce the risk of that happening. Using multiple validation sets isn't a bad idea, but the more validation sets you use, the longer the total training time will be.

<br><br>

# Data mismatch
Data mismatch can happen when the data that is used is not representative of what the model will be used for. To avoid this issue, we use another set of data, that is represenative of the traning set, called the test-dev set. This is again data that was taken from the training set.

The test-dev set is the first thing you should be testing the model on. If you get issues here, it means that you most likely have an underfitting or overfitting model. If this passes but the validation set does not, it likely means that you have a data mismatch.


