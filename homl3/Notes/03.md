# Chapter 3: Classification

## MNIST

MNIST is a dataset that consists of roughly 70,000 images of numbers alongside the answer

Sklearn offers a method that can retrieve the dataset:
```py
from sklearn.datasets import fetch_openml

mnist = fetch_openml('mnist_784', as_frame=False)
```
By default, fetch_openml will put the dataset in a dataframe, but since we're working with 28x28 image data, it's better to set it to False and get the default np array instead

Code to plot an image from a np array:
```py
import matplotlib.pyplot as plt

X, y = mnist.data, mnist.target

def plot_digit(image_data):
    image = image_data.reshape(28, 28)
    plt.imshow(image, cmap="binary")
    plt.axis("off")

some_digit = X[0]
plot_digit(some_digit)
plt.show() # Need a tool like like pycharm SciView
```

## Training a Binary Classifier
In order to prepare data for the model, these variables will contain True for all the values that are '5', False otherwise

```py
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]

# This syntax is only valid with np arrays
y_train_5 = (y_train == '5')
y_test_5 = (y_test == '5')
```

```py
from sklearn.linear_model import SGDClassifier

sgd_clf = SGDClassifier(random_state=42)
sgd_clf.fit(X_train, y_train_5)
```

## Performance Measures

### Measuring Accuracy Using Cross-Validation

Measuring accuracy with cross_val_score
```py
from sklearn.model_selection import cross_val_score

cvs = cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")
print(cvs) # 0.95035, 0.96035, 0.9604
```

How to use a dummy classifier (see docs for strategies)
```py
from sklearn.dummy import DummyClassifier

dummy_clf = DummyClassifier(strategy="prior") # always takes the most frequent
dummy_clf.fit(X_train, y_train_5)
cvs = cross_val_score(dummy_clf, X_train, y_train_5, cv=3, scoring="accuracy")
print(cvs) # 90.90965, 90.90965, 90.90965
```

The DummyClassifier will always take the most frequent value. This shows why accuracy may not be the best option for measuring performance

#### An equivalent function to cross_val_score
```py
from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone

skfolds = StratifiedKFold(n_splits=3)

for train_index, test_index in skfolds.split(X_train, y_train_5):
    clone_clf = clone(sgd_clf)
    X_train_folds = X_train[train_index]
    y_train_folds = y_train_5[train_index]
    X_test_fold = X_train[test_index]
    y_test_fold = y_train_5[test_index]

    clone_clf.fit(X_train_folds, y_train_folds)
    y_pred = clone_clf.predict(X_test_fold)
    n_correct = sum(y_pred == y_test_fold)
    print(n_correct / len(y_pred))
```

### Confusion Matrices
The idea of a confusion matrix is to keep track of the false positive, false negative, true positive and true negative results.

Example using is 5 or is not 5: 
- False positive: A false positive means it incorrectly detected the image as 5 
- False negative: A false negative means it classified a 5 as something else 
- True positive: A true positive means it correctly classified a 5 
- False positive: A true negative means it correctly classified a non-5 as not a 5

First, we need all the predictions:
```py
from sklearn.model_selection import cross_val_predict

y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5)
```

Now we can use the built-in method to calculate the confusion matrix
```py
from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_train_5, y_train_pred)
print(cm)
```

The precision value: (TP) / TP + FP
```py
from sklearn.metrics import precision_score

precision_score(y_train_5, y_train_pred)
```
Precision tells us exactly the ratio of correctly classified 5s compared to miss-classifications of 5

<br>

The recall value: (TP) / TP + FN
```py
from sklearn.metrics import recall_score

recall_score(y_train_5, y_train_pred)
```
Recall gives us the ratio of "how many 5s were detected" compared to "how many 5s were there"

<br>

The F1 score: (TP) / ((TP + (FN + FP)) / 2)
```py
from sklearn.metrics import f1_score

f1_score(y_train_5, y_train_pred)
```
The F1 score favors recall and precision valuey that are near each other. It gives us a more representative accuracy*

Another equivalent F1 formula:
2 * (precision * recall) / (precision + recall
)

### Precision and Recall

### The Precision/Recall Trade-off

### The ROC Curve

## Multiclass Classification

## Error Analysis

## Multilabel Classification

## Multioutput Classification

