# Starting a machine learning project
https://github.com/ageron/handson-ml3/blob/main/02_end_to_end_machine_learning_project.ipynb

See appendix_a.md for ML project checklist

## Frame the problem
Signal: "A piece of information fed to a machine learning sytstem is often called a *signal*"

Pipeline: A series of data processing components. See p.42

-> Doing this allows you to have a better idea of what you're being asked to do and how you will complete the task

-> Getting a better idea of why you are being asked to do said task will help you plan

## Select a performance mesure

### Notations:
- m = the number of instances in the dataset
- x(i) is a vector containing the information about the features
- y(i) contains the label
- X is a matrix that contains all the feature values
- h is the system's prediction system

## RMSE
See p.43 for equation

RMSE is an equation that calculates the performance of a model, but is usually used in regression tasks. RMSE follows the l2 norm and will give a higher weight to bigger errors

In more details, RMSE uses the Euclidian distance to calculate the distance between the target vector and the prediction vector.

## MAE
See p.45 for equation

Just like RMSE, MAE calculates the distance between the prediction vector and the target vector, but it uses the Manhattan distance instead. MAE is also an l1 norm, which means it doesn't give as much weight to bigger errors like RMSE does.

## lk norms
-> What is the letter? Weird p or weird cursive l ??

When talking about lk norms, a higher index, k, means that the function will give more weight to bigger errors, and the opposite for a smaller k. Which one you will use depends on what exactly you're trying to measure.

## Downloading the data using an automated Python function 
```py
from pathlib import Path
import pandas as pd
import tarfile
import urllib.request

def load_housing_data():
    tarball_path = Path("datasets/housing.tgz")
    if not tarball_path.is_file():
        Path("datasets").mkdir(parents=True, exist_ok=True)
        url = "https://github.com/ageron/data/raw/main/housing.tgz"
        urllib.request.urlretrieve(url, tarball_path)  # Download the data
        with tarfile.open(tarball_path) as housing_tarball:
            housing_tarball.extractall(path="datasets") # Extract from the tar.gz
    return pd.read_csv(Path("datasets/housing/housing.csv"))

housing = load_housing_data() # Load into a pd dataframe
```

## Creating a test set

```py
import numpy as np

def shuffle_and_split_data(data, test_ratio):
    shuffled_indices = np.random.permutation(len(data))
    test_set_size = int(len(data) * test_ratio)
    test_indices = shuffled_indices[:test_set_size]
    train_indices = shuffled_indices[test_set_size:]
    return data.iloc[train_indices], data.iloc[test_indices]

train_set, test_set = shuffle_and_split_data(housing, 0.2)
len(train_set)
```

### np.random.permutation
This method returns a random permutation of the given data. We passed a number in this case, so it will make a permutations of the range of that number

### test_set_size, test_indices and train_indices
test_set_indices decides how many indices is equal to the test_ratio, so in this case, 20640 * 0.2\
test_indices and train_indices take in the first 20% and the last 80% of the data. The data that it is taking is the one shuffled by np.random.permutation\
the function then returns the rows selected by train_indices and test_indices

### Using a hash shuffle and split method
Using a hash shuffle a split method allows us to assure that the training and test data are always seperated even if we feed more data into the dataset.
```py
from zlib import crc32

def is_id_in_test_set(identifier, test_ratio):
    return crc32(np.int64(identifier)) < test_ratio * 2**32

def split_data_with_id_hash(data, test_ratio, id_column):
    ids = data[id_column]
    in_test_set = ids.apply(lambda id_: is_id_in_test_set(id_, test_ratio))
    return data.loc[in_test_set], data.loc[in_test_set]
```

Having those randomized test/train split algorithms are good, but since it's randomized, they can introduce some sampling bias. This is where StraifiedShuffleSplit can be used. To do this, you can create a new category in the data using pd.cut and using big enough stratums (using bins), you can categorize an important factor. With a categorized column and StratifiedShuffleSplit, we can make sure that the data is less biased. This is because it'll split the categorized data to get almost even distribution of data in the training and in the test datasets.

```py
housing["income_cat"] = pd.cut(housing["median_income"],
                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
                               labels=[1, 2, 3, 4, 5])



from sklearn.model_selection import StratifiedShuffleSplit

splitter = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)
strat_splits = []
for train_index, test_index in splitter.split(housing, housing["income_cat"]):
    strat_train_set_n = housing.iloc[train_index]
    strat_test_set_n = housing.iloc[test_index]
    strat_splits.append([strat_train_set_n, strat_test_set_n])

strat_train_set, strat_test_set = strat_splits[0]

# To remove the column
for set_ in (strat_train_set, strat_test_set):
    set_.drop("income_cat", axis=1, inplace=True)
```

## Exploring the data
When manipulating the data for visualization, it's always a good idea to be working on a copy of the data
```py
housing = strat_train_set.copy()
```

### Visualizing Data
In a scatter plot, using the alpha parameter helps to visualize the places with a high density of data points

For more details on how to visualize a plot better, see the parameters used on p.62

### Look for Correlations
In order to get a good view of the correlations for a specific column, we can use 
```py
print(variable_storing_corr["column"].sort_values(ascending=false))
``` 

Note: the pd.cor() method **only calculates linear correlation**

### Experiment with Attribute Combinations
In order to get a better idea of what is going on in our data, we can combine some closely related attributes to get a more useful attribute. For example, the amount of bedrooms and the amount of households seperate aren't super useful, but the amount of bedrooms per household can be more interesting.

## Prepare the Data for Machine Learning Algorithmns

### Clean the Data
In the data that is being used in this project, there are some missing values in some of the columns.  We can fix this using an imputer from scikit-learn.

Here's an example of how to use the SimpleImputer imputer from sklearn.impute:
```py
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy="median") # There are other available strategies, this one only works on numerical attributes
housing_num = housing.select_dtypes(include=[np.number]) # Only select numerical attributes
imputer.fit(housing_num)
X = imputer.transform(housing_num) # X is a numpy array, not a pandas dataframe
housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index) # data is a 2d numpy array, index is the row number
```

Also, see sklearn_design.md

### Handling Text and Categorical Attributes
When dealing with text attributes, it's possible to convert them into numbers using an encoder. Here's 2 ways to encode that data:

Using OrdinalEncoder. This encoder gives a number attribute to each category, but this can cause issues when a machine learning algorithm interprets a bigger gap bewteen the numbers as a higher weight
```py
from sklearn.preprocessing import OrdinalEncoder

housing_cat = housing[["ocean_proximity"]]
ordinal_encoder = OrdinalEncoder()
housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)
```

Using OneHotEncoder. This encoder gives the number 1 to the active category and 0 to the rest, avoiding the previous issue.
```py
from sklearn.preprocessing import OneHotEncoder

housing_cat = housing[["ocean_proximity"]]
cat_encoder = OneHotEncoder()
housing_cat_1hot = cat_encoder.fit_transform(housing_cat)
```
In estimators fit with a DataFrame, the column names are stored in the features_names_in_ attribute

There is also the function pd.get_dummies(dataframe). This also does the same as OneHotEncoder, but it does not remember what categories it was trained on.

### Feature Scaling and Transformation
Feature Scaling and Transformation consists of modifying data in order to avoid issues such as big differences in scales between attributes. Issues like that can hinder the performance of a machine learning model.

#### MinMaxScaler (Aka normalization)

This consists of substracting the min value and dividing it by the difference between the min and the max. It is possible to get a different range other than (0, 1) with MinMaxScaler via the feature_range parameter.

This method is more sensitive to outlier values and is sensitive to a heavy tail.

#### StandardScaler (Aka standardization)

This consists of subtracting the mean value then dividing by the standard deviation. 

This method is less sensitive to outlier values but is sensitive to a heavy tail.

#### Heavy Tail

A heavy tail is when values far from the mean are not exponentially rare. This can cause Scaler methods, such as the two above, to put everything into a smaller range.

There are a few options when it comes to fixing a heavy tail:

#### Log, sqrt, etc

#### Bucketing

Bucketing consists of dividing the feature into equal-ish sized buckets and replacing the feature value to the value of the bucket it belongs to. Similar to the income_cat stratum. This leads to a much more uniform distribution

There are other ways to use bucketing, like in multimodal distributions.  **

#### RBF (Radial Basis Function, commonly Gaussian RBF)

RBF consists of finding the distance between the input value and a fixed point. This can be used to measure similarity (and also results in a bell curve). A smaller gamma given to the RBF function means that the variance will be higher (gamma is represented by y in the formula)

exp(-y(x - fixed_point)^2)

#### Reversing a transformation

When using a technique, such as calculating the log of a set of features, to avoid a heavy tail, it'll lead the model to predict the log of the wanted value. This means that we have to calculate the exponential of the result in order to get the value that we are looking for. This is also possible to do with data transformed by sklearn methods, it can be reversed with the inverse_tranform() method

We can also use the TransformedTargetRegressor method from sklearn to train a model and inverse the result in less steps

```py
from sklearn.linear_model import LinearRegression
from sklearn.compose import TransformedTargetRegressor
from sklearn.preprocessing import StandardScaler

# housing_labels is the median_house_value category

# Example without TransformedTargetRegressor

target_scaler = StandardScaler()
scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())

model = LinearRegression()
model.fit(housing[["median_income"]], scaled_labels)
some_new_data = housing[["median_income"]].iloc[:5]

scaled_predictions = model.predict(some_new_data)
predictions = target_scaler.inverse_transform(scaled_predictions)

# With TransformedTargetRegressor

model = TransformedTargetRegressor(LinearRegression(), transformer=StandardScaler())
model.fit(housing[["median_income"]], housing_labels)
predictions = model.predict(some_new_data)

```

### Custom Transformers

There are two ways to make custom transformers.

Option 1: from sklearn.preprocessing import FunctionTransformer

FunctionTransformer allows us to create a custom transformer that doesn't require any training.

EX:
```py
from sklearn.preprocessing import FunctionTransformer

log_transformer = FunctionTransformer(np.log, inverse_func=np.exp) # The first parameter is the function that will be called on the data
log_pop = log_transformer.transform(housing[["population"]])
```

Option 2: Creating a new transformer via classes

```py

# NOTE: this is not a complete implementation
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_array, check_is_fitted

class StandardScalerClone(BaseEstimator, TransformerMixin):
    def __init__(self, with_mean=True):
        self.with_mean = with_mean

    def fit(self, X, y=None):
        X = check_array(X)
        self.mean_ = X.mean(axis=0)
        self.scale_ = X.std(axis=0)
        self.n_features_in_ = X.shape[1]
        return self # fit() always returns self

    def transform(self, X):
        check_is_fitted(self)
        X = check_array(X)
        asset self.n_features_in_ == X.shape[1]
        if self.with_mean:
            X = X - self.mean_
        return X / self.scale_
```

- fit is required to have X and y, set y = None if y is not needed
- TransformerMixin creates fit_tranform() for us
- BaseEstimator gives us the get_params() and set_params() methods if *args and **kwargs are not used in the constructor 


### Transformation Pipelines

There are a few ways that we can create our own transformation pipelines for our use cases.

We can use make_pipeline:
```py
from sklearn.pipeline import make_pipeline

pipeline = make_pipeline(
    SimpleImputer(strategy="median"),
    StandardScaler()
)

"""
We can use as many transformers as we need. If the final operation is a predictor, it will be treated as such.
"""
```

We can use Pipeline:
```py
from sklearn.pipeline import Pipeline

num_pipeline = Pipeline([
    ("impute", SimpleImputer(strategy="median")),
    ("standardize", StandardScaler())
])
```

Then we can use ColumnTransformer to set which feature should be treated by what pipeline
```py
from sklearn.compose import ColumnTransformer

preprocessing = ColumnTransformer([
    ("name", pipeline, features),
    ("bedrooms", ratio_pipeline(), ["total_bedrooms", "total_rooms"])
], remainder=default_pipeline)
```

## Select and Train a Model

### Train and Evaluate on the Training set

Using the pipeline defined above (preprocessing), we can use make_pipeline to train our selected model
```py
lin_reg = make_pipeline(preprocessing, LinearRegression()) # Can be any other model
lin_reg.fit(housing, housing_labels)

print(lin_reg.predict())
```

We can also use methods, such as RMSE, to calculate the error:
```py
from sklearn.metrics import root_mean_squared_error

rmse = root_mean_squared_error(housing_labels, housing_predictions)
print(rmse)
```

### Better Evaluation Using Cross-Validation

We can use built-in methods, such as cross_val_score, to get a better representation of the error in the model.

```py
from sklearn.model_selection import cross_val_score

rmses = -cross_val_score(tree_reg, housing, housing_labels, scoring="neg_root_mean_squared_error", cv=10)
```

This can take a long time, as it will train the model cv times

## Fine-Tune Your Model

### Grid Search

### Randomized Search

### Ensemble Methods

### Analyzing the Best Models and Their Errors

### Evaluate Your System on the Test Set

## Launch, Monitor, And Maintain Your System